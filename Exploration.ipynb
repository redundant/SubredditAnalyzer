{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "from pymongo import MongoClient\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = MongoClient(\"3.16.43.190\")\n",
    "db = client.redditproject\n",
    "\n",
    "db.authenticate(\"redundant\",\"monkey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "everything_filter = {\"body\":{\"$nin\":[\"[deleted]\",\"[removed]\"]}}\n",
    "td_filter = {\"body\":{\"$nin\":[\"[deleted]\",\"[removed]\"]},\"sub\":\"the_donald\"}\n",
    "fc_filter = {\"body\":{\"$nin\":[\"[deleted]\",\"[removed]\"]},\"sub\":\"fullcommunism\"}\n",
    "con_filter = {\"body\":{\"$nin\":[\"[deleted]\",\"[removed]\"]},\"sub\":\"conservative\"}\n",
    "lsc_filter =  {\"body\":{\"$nin\":[\"[deleted]\",\"[removed]\"]},\"sub\":\"latestagecapitalism\"}\n",
    "pol_filter = {\"body\":{\"$nin\":[\"[deleted]\",\"[removed]\"]},\"sub\":\"politics\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "td_corpus = [document[\"body\"] for document in db.redditproject.find(td_filter,{\"_id\":0,\"body\":1})]\n",
    "fc_corpus = [document[\"body\"] for document in db.redditproject.find(fc_filter,{\"_id\":0,\"body\":1})]\n",
    "con_corpus = [document[\"body\"] for document in db.redditproject.find(con_filter,{\"_id\":0,\"body\":1})]\n",
    "lsc_corpus = [document[\"body\"] for document in db.redditproject.find(lsc_filter,{\"_id\":0,\"body\":1})]\n",
    "pol_corpus = [document[\"body\"] for document in db.redditproject.find(pol_filter,{\"_id\":0,\"body\":1})]\n",
    "#corpus = [(document[\"body\"],document[\"sub\"]) for document in db.redditproject.find({},{\"_id\":0,\"body\":1,\"sub\":1})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(input_string):\n",
    "        if '#Welcome to r/LateStageCapitalism\\n***\\n\\n#' in input_string:\n",
    "                            return \"\"\n",
    "        if \"Your post was removed\" in input_string:\n",
    "            return \"\"\n",
    "        \n",
    "        clean_s = re.sub(\"\\(http.*\\)\",\"\",input_string)\n",
    "        clean_s = re.sub(\"/./\",\"\", clean_s)\n",
    "        clean_s = re.sub(\"r/\",\"\", clean_s)\n",
    "        clean_s = re.sub(\"-\",\" \", clean_s)\n",
    "        clean_s = clean_s.translate(str.maketrans(\"\\n\",\" \",string.punctuation))\n",
    "        clean_s = re.sub(\"^[rR][eeEE][eE]*\",\"ree\",clean_s)\n",
    "        clean_s = clean_s.lower()\n",
    "        clean_s = re.sub(\"[^a-zA-Z ]*\",\"\", clean_s)\n",
    "        clean_s = re.sub(\" +\",\" \", clean_s)\n",
    "        clean_s = re.sub(\"http[a-zA-Z]*\",\"\",clean_s)\n",
    "        clean_s = re.sub(\"(maga)+\",\"maga\",clean_s)\n",
    "        clean_s = re.sub(\"isnt \",\" \",clean_s)\n",
    "        clean_s = re.sub(\"was \",\" \",clean_s)\n",
    "        clean_s = re.sub('labour',\"labor\",clean_s)\n",
    "        clean_s = re.sub(\"hilary\", \"hillary\",clean_s)\n",
    "        clean_s = re.sub(\"uspez\",\"spez\", clean_s)\n",
    "\n",
    "        return clean_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "td_clean_corpus = [clean_string(document) for document in td_corpus]\n",
    "con_clean_corpus = [clean_string(document) for document in con_corpus]\n",
    "pol_clean_corpus = [clean_string(document) for document in pol_corpus]\n",
    "lsc_clean_corpus = [clean_string(document) for document in lsc_corpus]\n",
    "fc_clean_corpus = [clean_string(document) for document in fc_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_corpus = td_clean_corpus + con_clean_corpus + pol_clean_corpus + lsc_clean_corpus + fc_clean_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedditCommentProcessor:\n",
    "    def __init__(self,stemmer,vectorizer):\n",
    "        self.stemmer=stemmer\n",
    "        self.vectorizer = vectorizer\n",
    "    \n",
    "    def fit(self, corpus):\n",
    "        clean_corpus = list(filter(self.__space_or_empty_filter__,corpus))\n",
    "        \n",
    "        lem_corpus = [\" \".join([self.stemmer.lemmatize(word) for word in clean_s.split(\" \")]) for clean_s in clean_corpus]\n",
    "        self.vectorizer.fit(lem_corpus)\n",
    "    \n",
    "    def __space_or_empty_filter__(self, string):\n",
    "        if string.isspace():\n",
    "            return False\n",
    "        if string ==\"\":\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        return self.vectorizer.get_feature_names()\n",
    "    \n",
    "    def transform(self, corpus):\n",
    "        clean_corpus = list(filter(self.__space_or_empty_filter__,corpus))\n",
    "        lem_corpus = [\" \".join([self.stemmer.lemmatize(word) for word in clean_s.split(\" \")]) for clean_s in clean_corpus]\n",
    "\n",
    "        return self.vectorizer.transform(lem_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")+[\"tldr\",\"shit\",\"fuck\",\"wasnt\",\"using\",\"ago\",\"around\",\"fucking\",\"mr\",\"literally\",\"suck\",\"push\",\"time\", \"actually\",\"theyre\",\"think\",\"said\",\"never\", \"every\", \"long\", \"new\", \"day\", \"last\", \"back\", \"take\", \"first\", \"next\",\"need\", \"much\", \"white\", \"well\", \"mean\", \"could\", \"someone\", \"many\",\"make\", \"even\",\"know\", \"cant\", \"something\", \"say\", \"want\", \"way\", \"see\", \"doesnt\", \"point\", \"good\",\"hey\",\"like\",\"dont\",\"ive\",\"still\",\"doe\",\"yeah\",\"really\",\"lot\",\"one\",\"le\",\"would\",\"ha\",\"fact\",\"sure\",\"man\",\"didnt\",\"nd\",\"people\",\"link\",\"thing\",\"youre\",\"also\",\"thats\",\"im\",\"go\",\"year\",\"going\",\"get\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc = RedditCommentProcessor(WordNetLemmatizer(), CountVectorizer(stop_words=stop_words,min_df=100, max_df=.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc.fit(clean_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "td_matrix = proc.transform(td_clean_corpus)\n",
    "fc_matrix = proc.transform(fc_clean_corpus)\n",
    "pol_matrix = proc.transform(pol_clean_corpus)\n",
    "con_matrix = proc.transform(con_clean_corpus)\n",
    "lsc_matrix = proc.transform(lsc_clean_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = proc.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I tried LSA, but NMF seemed to give more coherent topics, along with the benefit of interpretability (no negative weights on words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "td_nmf = NMF(10,init=\"nndsvd\")\n",
    "td_topics = td_nmf.fit_transform(td_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_nmf = NMF(10,init=\"nndsvd\")\n",
    "con_topics = con_nmf.fit_transform(con_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsc_nmf = NMF(10,init=\"nndsvd\")\n",
    "lsc_topics = lsc_nmf.fit_transform(lsc_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_nmf = NMF(10,init=\"nndsvd\")\n",
    "fc_topics = fc_nmf.fit_transform(fc_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol_nmf = NMF(10,init=\"nndsvd\")\n",
    "pol_topics = pol_nmf.fit_transform(pol_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(td_nmf, feature_names, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(fc_nmf, feature_names,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(lsc_nmf, feature_names,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(con_nmf, feature_names,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(pol_nmf, feature_names,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's graphically look at the similarity of different topics between these communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(top_1,top_2):\n",
    "    return np.dot(top_1,top_2)/(np.linalg.norm(top_1)*np.linalg.norm(top_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_similarity(sub_1,sub_2,same=False):\n",
    "    \n",
    "    for i in range(10):\n",
    "        for j in range(0,10):\n",
    "            c_s = cosine_similarity(sub_1[i],sub2[j])\n",
    "            if c_s > 1e-1:\n",
    "                print(\"Topics ({},{}) are {} similar\".format(i,j,c_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap_generator(sub_1,sub_2, same=False):\n",
    "    a=np.zeros((10,10))\n",
    "    for i in range(10):\n",
    "        for j in range(0,10):\n",
    "            c_s = cosine_similarity(sub_1[i],sub_2[j])\n",
    "            a[i][j]=c_s\n",
    "            if same and i==j:\n",
    "                a[i][j]=0\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.heatmap(heatmap_generator(td_nmf.components_,con_nmf.components_,False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here I'm interested in how next to no topics are in common between The_Donald and conservative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(heatmap_generator(pol_nmf.components_,con_nmf.components_,False))\n",
    "plt.xlabel(\"Conservative\")\n",
    "plt.ylabel(\"Politics\")\n",
    "plt.title(\"Topic Similarity (Brighter = Higher)\")\n",
    "plt.savefig(\"pol_con.png\",dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here the two bright squares at 1,3 and 2,1 are interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.heatmap(heatmap_generator(pol_nmf.components_,lsc_nmf.components_,False))\n",
    "plt.xlabel(\"Late Stage Capitalism\")\n",
    "plt.ylabel(\"Politics\")\n",
    "plt.title(\"Topic Similarity (Brighter = Higher)\")\n",
    "plt.savefig(\"pol_lsc.png\",dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic 0 of Late Stage Capitalism and topic 5 of Politics have some interesting features (i.e. bright lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.heatmap(heatmap_generator(con_nmf.components_,lsc_nmf.components_,False))\n",
    "plt.xlabel(\"Late Stage Capitalism\")\n",
    "plt.ylabel(\"Conservative\")\n",
    "plt.title(\"Topic Similarity (Brighter = Higher)\")\n",
    "plt.savefig(\"con_lsc.png\",dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly here topic 0 and topic 7 overlap well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(heatmap_generator(lsc_nmf.components_,fc_nmf.components_,False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hmm, looks like Full communism doesn't really overlap with late stage capitalism. This is curious ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
